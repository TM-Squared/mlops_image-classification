from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.mysql.hooks.mysql import MySqlHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from datetime import datetime, timedelta
import sys
import os
import pandas as pd

sys.path.append('/opt/airflow/ml/training')

def check_data_availability(**context):
    """V√©rifier si suffisamment de donn√©es sont disponibles pour l'entra√Ænement"""
    mysql_hook = MySqlHook(mysql_conn_id='mysql_default')
    
    # Compter les images disponibles par classe
    query = """
    SELECT label, COUNT(*) as count
    FROM plants_data 
    WHERE url_s3 IS NOT NULL AND image_exists = TRUE
    GROUP BY label
    """
    
    try:
        df = mysql_hook.get_pandas_df(query)
        
        if df.empty:
            print("‚ö†Ô∏è Aucune donn√©e dans la base, utilisation des donn√©es par d√©faut")
            return {
                'total_images': 60,  # Valeur par d√©faut
                'classes': [
                    {'label': 'dandelion', 'count': 30},
                    {'label': 'grass', 'count': 30}
                ],
                'data_source': 'default'
            }
        
        print("üìä Donn√©es disponibles:")
        for _, row in df.iterrows():
            print(f"  - {row['label']}: {row['count']} images")
        
        # V√©rifier qu'on a au moins 10 images de chaque classe
        min_images_per_class = 10
        for _, row in df.iterrows():
            if row['count'] < min_images_per_class:
                print(f"‚ö†Ô∏è Peu d'images pour {row['label']}: {row['count']} < {min_images_per_class}")
        
        total_images = df['count'].sum()
        print(f"‚úÖ {total_images} images disponibles pour l'entra√Ænement")
        
        return {
            'total_images': int(total_images),
            'classes': df.to_dict('records'),
            'data_source': 'database'
        }
        
    except Exception as e:
        print(f"‚ùå Erreur acc√®s base de donn√©es: {e}")
        # Retourner des valeurs par d√©faut
        return {
            'total_images': 60,
            'classes': [
                {'label': 'dandelion', 'count': 30},
                {'label': 'grass', 'count': 30}
            ],
            'data_source': 'default'
        }

def prepare_training_data(**context):
    """Pr√©parer les donn√©es d'entra√Ænement depuis la base"""
    
    ti = context['ti']
    data_check = ti.xcom_pull(task_ids='check_data_availability')
    
    if data_check['data_source'] == 'default':
        print("üîÑ Utilisation des donn√©es par d√©faut")
        return {
            'training_mode': 'default_urls',
            'total_samples': 60,
            'data_source': 'URLs par d√©faut'
        }
    
    try:
        mysql_hook = MySqlHook(mysql_conn_id='mysql_default')
        
        # R√©cup√©rer toutes les donn√©es disponibles
        query = """
        SELECT url_s3, label 
        FROM plants_data 
        WHERE url_s3 IS NOT NULL AND image_exists = TRUE
        ORDER BY RAND()
        LIMIT 100
        """
        
        df = mysql_hook.get_pandas_df(query)
        
        if df.empty:
            print("‚ö†Ô∏è Aucune donn√©e trouv√©e, utilisation du mode par d√©faut")
            return {
                'training_mode': 'default_urls',
                'total_samples': 60,
                'data_source': 'URLs par d√©faut'
            }
        
        # Convertir les URLs S3 en cl√©s S3
        s3_keys = []
        labels = []
        
        for _, row in df.iterrows():
            url_s3 = row['url_s3']
            label = row['label']
            
            # Convertir s3://raw-data/raw/dandelion/00000000.jpg en raw/dandelion/00000000.jpg
            if url_s3.startswith('s3://raw-data/'):
                s3_key = url_s3.replace('s3://raw-data/', '')
                s3_keys.append(s3_key)
                labels.append(label)
        
        print(f"üìã {len(s3_keys)} images pr√©par√©es pour l'entra√Ænement")
        
        return {
            'training_mode': 'minio_keys',
            's3_keys': s3_keys,
            'labels': labels,
            'total_samples': len(s3_keys),
            'data_source': 'MinIO via Base'
        }
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la pr√©paration des donn√©es: {e}")
        return {
            'training_mode': 'default_urls',
            'total_samples': 60,
            'data_source': 'URLs par d√©faut (erreur)'
        }

def train_model(**context):
    """Entra√Æner le mod√®le"""
    
    ti = context['ti']
    training_data = ti.xcom_pull(task_ids='prepare_training_data')
    
    if not training_data:
        raise ValueError("‚ùå Aucune donn√©e d'entra√Ænement re√ßue")
    
    print(f"üöÄ D√©but de l'entra√Ænement")
    print(f"  - Mode: {training_data['training_mode']}")
    print(f"  - √âchantillons: {training_data['total_samples']}")
    print(f"  - Source: {training_data['data_source']}")
    
    # Importer le trainer
    from trainer import train_from_s3_keys, train_from_database_minio
    
    try:
        if training_data['training_mode'] == 'minio_keys':
            # Entra√Æner avec les cl√©s S3
            result = train_from_s3_keys(
                training_data['s3_keys'],
                training_data['labels'],
                num_epochs=5
            )
        else:
            # Entra√Æner avec les donn√©es par d√©faut
            result = train_from_database_minio(num_epochs=5)
        
        print(f"‚úÖ Entra√Ænement termin√©:")
        print(f"  - Pr√©cision: {result['accuracy']:.2%}")
        print(f"  - √âchantillons: {result['num_samples']}")
        print(f"  - Stockage: {result.get('storage', 'MinIO')}")
        
        # Informations du mod√®le
        model_info = result.get('model_info', {})
        if model_info:
            print(f"  - Mod√®le: {model_info.get('key', 'N/A')}")
            print(f"  - Taille: {model_info.get('size', 'N/A')} bytes")
        
        return result
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'entra√Ænement: {e}")
        raise

def evaluate_model(**context):
    """√âvaluer le mod√®le entra√Æn√©"""
    
    ti = context['ti']
    training_result = ti.xcom_pull(task_ids='train_model')
    
    if not training_result:
        raise ValueError("‚ùå Aucun r√©sultat d'entra√Ænement re√ßu")
    
    accuracy = training_result['accuracy']
    min_accuracy = 0.6  # 60% minimum
    
    print(f"üîç √âvaluation du mod√®le:")
    print(f"  - Pr√©cision obtenue: {accuracy:.2%}")
    print(f"  - Seuil minimum: {min_accuracy:.2%}")
    print(f"  - Stockage: {training_result.get('storage', 'MinIO')}")
    
    # Informations du mod√®le
    model_info = training_result.get('model_info', {})
    if model_info:
        print(f"  - Mod√®le stock√©: {model_info.get('key', 'N/A')}")
        print(f"  - Format: {model_info.get('format', 'N/A')}")
        print(f"  - Date: {model_info.get('last_modified', 'N/A')}")
    
    if accuracy >= min_accuracy:
        print("‚úÖ Mod√®le valid√© pour le d√©ploiement")
        return {
            'status': 'APPROVED',
            'accuracy': accuracy,
            'model_info': model_info,
            'storage': training_result.get('storage', 'MinIO')
        }
    else:
        print("‚ùå Mod√®le rejet√© (pr√©cision insuffisante)")
        return {
            'status': 'REJECTED',
            'accuracy': accuracy,
            'reason': f'Pr√©cision {accuracy:.2%} < {min_accuracy:.2%}',
            'model_info': model_info,
            'storage': training_result.get('storage', 'MinIO')
        }

def deploy_model(**context):
    """D√©ployer le mod√®le approuv√©"""
    
    ti = context['ti']
    evaluation_result = ti.xcom_pull(task_ids='evaluate_model')
    
    if evaluation_result['status'] == 'APPROVED':
        print("üöÄ D√©ploiement du mod√®le approuv√©")
        
        model_info = evaluation_result.get('model_info', {})
        storage = evaluation_result.get('storage', 'MinIO')
        
        print(f"üì¶ Mod√®le d√©ploy√© avec succ√®s")
        print(f"  - Stockage: {storage}")
        print(f"  - Pr√©cision: {evaluation_result['accuracy']:.2%}")
        
        if model_info:
            print(f"  - Fichier: {model_info.get('key', 'N/A')}")
            print(f"  - Format: {model_info.get('format', 'N/A')}")
            print(f"  - Taille: {model_info.get('size', 'N/A')} bytes")
        
        return {
            'status': 'DEPLOYED',
            'model_info': model_info,
            'accuracy': evaluation_result['accuracy'],
            'storage': storage,
            'deployment_time': datetime.now().isoformat()
        }
    else:
        print("‚ùå D√©ploiement annul√© - mod√®le non approuv√©")
        return {
            'status': 'DEPLOYMENT_CANCELLED',
            'reason': evaluation_result.get('reason', 'Validation failed'),
            'accuracy': evaluation_result['accuracy']
        }

def send_notification(**context):
    """Envoyer une notification de fin de pipeline"""
    
    ti = context['ti']
    
    # R√©cup√©rer les r√©sultats de toutes les t√¢ches
    data_check = ti.xcom_pull(task_ids='check_data_availability')
    training_result = ti.xcom_pull(task_ids='train_model')
    evaluation_result = ti.xcom_pull(task_ids='evaluate_model')
    deployment_result = ti.xcom_pull(task_ids='deploy_model')
    
    print("üìß Notification de fin de pipeline:")
    print(f"  - Donn√©es: {data_check['total_images']} images ({data_check['data_source']})")
    print(f"  - Entra√Ænement: {training_result['accuracy']:.2%} de pr√©cision")
    print(f"  - √âvaluation: {evaluation_result['status']}")
    print(f"  - D√©ploiement: {deployment_result['status']}")
    print(f"  - Stockage: {training_result.get('storage', 'MinIO')}")
    
    # Informations du mod√®le
    model_info = training_result.get('model_info', {})
    if model_info:
        print(f"  - Mod√®le: {model_info.get('key', 'N/A')}")
    
    # Simuler l'envoi d'une notification
    notification_data = {
        'pipeline': 'model_training_pipeline',
        'status': 'SUCCESS' if deployment_result['status'] == 'DEPLOYED' else 'PARTIAL_SUCCESS',
        'accuracy': training_result['accuracy'],
        'storage': training_result.get('storage', 'MinIO'),
        'timestamp': datetime.now().isoformat()
    }
    
    print(f"üì® Notification envoy√©e: {notification_data}")
    
    return "NOTIFICATION_SENT"

# Configuration du DAG
default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'model_training_pipeline',
    default_args=default_args,
    description='Pipeline complet d\'entra√Ænement de mod√®le avec MinIO',
    schedule_interval=None,  # D√©clenchement manuel
    catchup=False,
    tags=['ml', 'training', 'tensorflow', 'minio', 'production'],
    max_active_runs=1,
    doc_md="""
    ## Pipeline d'entra√Ænement de mod√®le avec MinIO
    
    Ce DAG orchestre l'entra√Ænement complet d'un mod√®le de classification d'images
    avec stockage sur MinIO.
    
    ### √âtapes:
    1. **check_data_availability**: V√©rifie la disponibilit√© des donn√©es
    2. **prepare_training_data**: Pr√©pare les donn√©es d'entra√Ænement
    3. **train_model**: Entra√Æne le mod√®le TensorFlow
    4. **evaluate_model**: √âvalue les performances
    5. **deploy_model**: D√©ploie le mod√®le si approuv√©
    6. **send_notification**: Envoie une notification de fin
    
    ### Fonctionnalit√©s:
    - Stockage des mod√®les sur MinIO
    - Fallback sur donn√©es par d√©faut si base indisponible
    - Versionning automatique des mod√®les
    - M√©triques d√©taill√©es avec MLflow
    
    ### Pr√©requis:
    - MinIO configur√© avec bucket 'models'
    - Base de donn√©es MySQL avec table plants_data
    - MLflow pour le tracking
    """
)

# D√©finition des t√¢ches
check_data_task = PythonOperator(
    task_id='check_data_availability',
    python_callable=check_data_availability,
    provide_context=True,
    dag=dag,
    doc_md="V√©rifie la disponibilit√© des donn√©es d'entra√Ænement"
)

prepare_data_task = PythonOperator(
    task_id='prepare_training_data',
    python_callable=prepare_training_data,
    provide_context=True,
    dag=dag,
    doc_md="Pr√©pare les donn√©es pour l'entra√Ænement"
)

train_model_task = PythonOperator(
    task_id='train_model',
    python_callable=train_model,
    provide_context=True,
    dag=dag,
    doc_md="Entra√Æne le mod√®le TensorFlow avec sauvegarde MinIO"
)

evaluate_model_task = PythonOperator(
    task_id='evaluate_model',
    python_callable=evaluate_model,
    provide_context=True,
    dag=dag,
    doc_md="√âvalue les performances du mod√®le"
)

deploy_model_task = PythonOperator(
    task_id='deploy_model',
    python_callable=deploy_model,
    provide_context=True,
    dag=dag,
    doc_md="D√©ploie le mod√®le si valid√©"
)

notify_task = PythonOperator(
    task_id='send_notification',
    python_callable=send_notification,
    provide_context=True,
    dag=dag,
    doc_md="Envoie une notification de fin de pipeline"
)

# D√©finir les d√©pendances
check_data_task >> prepare_data_task >> train_model_task >> evaluate_model_task >> deploy_model_task >> notify_task