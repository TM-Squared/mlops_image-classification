from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.mysql.hooks.mysql import MySqlHook
from datetime import datetime, timedelta
import sys
import os
import requests
import json

sys.path.append('/opt/airflow/ml/models')
sys.path.append('/opt/airflow/ml/training')

def load_test_data(**context):
    """Charger des donnÃ©es de test"""
    try:
        mysql_hook = MySqlHook(mysql_conn_id='mysql_default')
        
        # Prendre 20 images de test
        query = """
        SELECT url_s3, label 
        FROM plants_data 
        WHERE url_s3 IS NOT NULL AND image_exists = TRUE
        ORDER BY RAND()
        LIMIT 20
        """
        
        df = mysql_hook.get_pandas_df(query)
        
        if df.empty:
            print("âš ï¸ Aucune donnÃ©e de test dans la base, utilisation d'URLs par dÃ©faut")
            
            # URLs par dÃ©faut
            base_url = "https://raw.githubusercontent.com/btphan95/greenr-airflow/refs/heads/master/data"
            test_data = [
                {"url": f"{base_url}/dandelion/00000010.jpg", "label": "dandelion"},
                {"url": f"{base_url}/dandelion/00000011.jpg", "label": "dandelion"},
                {"url": f"{base_url}/dandelion/00000012.jpg", "label": "dandelion"},
                {"url": f"{base_url}/grass/00000010.jpg", "label": "grass"},
                {"url": f"{base_url}/grass/00000011.jpg", "label": "grass"},
                {"url": f"{base_url}/grass/00000012.jpg", "label": "grass"},
            ]
            
            return {
                'test_data': test_data,
                'data_source': 'default_urls'
            }
        
        # Convertir les URLs S3 en URLs HTTP pour les tests
        test_data = []
        for _, row in df.iterrows():
            url_s3 = row['url_s3']
            label = row['label']
            
            # Convertir s3://raw-data/raw/dandelion/00000000.jpg en URL HTTP
            if url_s3.startswith('s3://raw-data/'):
                # Pour les tests, utiliser les URLs GitHub Ã  la place
                filename = url_s3.split('/')[-1]
                category = url_s3.split('/')[-2]
                base_url = "https://raw.githubusercontent.com/btphan95/greenr-airflow/refs/heads/master/data"
                http_url = f"{base_url}/{category}/{filename}"
                
                test_data.append({
                    "url": http_url,
                    "label": label,
                    "s3_key": url_s3.replace('s3://raw-data/', '')
                })
        
        print(f"ðŸ“‹ {len(test_data)} images de test chargÃ©es depuis la base")
        
        return {
            'test_data': test_data,
            'data_source': 'database'
        }
        
    except Exception as e:
        print(f"âŒ Erreur chargement donnÃ©es test: {e}")
        
        # Fallback vers des URLs par dÃ©faut
        base_url = "https://raw.githubusercontent.com/btphan95/greenr-airflow/refs/heads/master/data"
        test_data = [
            {"url": f"{base_url}/dandelion/00000010.jpg", "label": "dandelion"},
            {"url": f"{base_url}/grass/00000010.jpg", "label": "grass"},
        ]
        
        return {
            'test_data': test_data,
            'data_source': 'fallback_urls'
        }

def evaluate_current_model(**context):
    """Ã‰valuer le modÃ¨le actuel via l'API"""
    ti = context['ti']
    test_result = ti.xcom_pull(task_ids='load_test_data')
    
    if not test_result:
        raise ValueError("âŒ Aucune donnÃ©e de test reÃ§ue")
    
    test_data = test_result['test_data']
    data_source = test_result['data_source']
    
    print(f"ðŸ” Ã‰valuation du modÃ¨le avec {len(test_data)} images")
    print(f"ðŸ“Š Source des donnÃ©es: {data_source}")
    
    # Tester la disponibilitÃ© de l'API
    try:
        response = requests.get("http://api:8000/health", timeout=10)
        if response.status_code != 200:
            raise Exception(f"API non disponible: {response.status_code}")
        
        api_info = response.json()
        print(f"âœ… API accessible: {api_info.get('framework', 'N/A')} - {api_info.get('tf_version', 'N/A')}")
        
    except Exception as e:
        print(f"âŒ API non accessible: {e}")
        print("ðŸ”„ Tentative d'Ã©valuation locale...")
        return evaluate_model_locally(test_data)
    
    # Ã‰valuer via l'API
    return evaluate_model_via_api(test_data)

def evaluate_model_via_api(test_data):
    """Ã‰valuer le modÃ¨le via l'API"""
    
    correct_predictions = 0
    total_predictions = 0
    predictions = []
    failed_requests = 0
    
    print("ðŸŒ Ã‰valuation via API")
    
    for i, test_item in enumerate(test_data):
        try:
            # Faire une prÃ©diction via l'API
            response = requests.post(
                "http://api:8000/predict-url",
                json={"image_url": test_item["url"]},
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                predicted_label = result['predicted_class']
                true_label = test_item['label']
                confidence = result['confidence']
                
                is_correct = predicted_label == true_label
                if is_correct:
                    correct_predictions += 1
                
                total_predictions += 1
                
                predictions.append({
                    'test_id': i + 1,
                    'url': test_item['url'],
                    'true_label': true_label,
                    'predicted_label': predicted_label,
                    'confidence': confidence,
                    'correct': is_correct
                })
                
                print(f"Test {i+1}: {true_label} -> {predicted_label} ({'âœ…' if is_correct else 'âŒ'}) - {confidence:.2%}")
                
            else:
                print(f"âŒ Erreur API pour test {i+1}: {response.status_code}")
                failed_requests += 1
                
        except Exception as e:
            print(f"âŒ Erreur test {i+1}: {e}")
            failed_requests += 1
    
    # Calculer les mÃ©triques
    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
    success_rate = total_predictions / len(test_data) if len(test_data) > 0 else 0
    
    print(f"\nðŸ“Š RÃ©sultats de l'Ã©valuation API:")
    print(f"  - Tests rÃ©ussis: {total_predictions}/{len(test_data)} ({success_rate:.2%})")
    print(f"  - PrÃ©dictions correctes: {correct_predictions}/{total_predictions}")
    print(f"  - PrÃ©cision: {accuracy:.2%}")
    print(f"  - RequÃªtes Ã©chouÃ©es: {failed_requests}")
    
    return {
        'evaluation_method': 'api',
        'accuracy': accuracy,
        'total_tests': len(test_data),
        'successful_tests': total_predictions,
        'correct_predictions': correct_predictions,
        'failed_requests': failed_requests,
        'success_rate': success_rate,
        'predictions': predictions
    }

def evaluate_model_locally(test_data):
    """Ã‰valuer le modÃ¨le localement (fallback)"""
    
    print("ðŸ  Ã‰valuation locale du modÃ¨le")
    
    try:
        # Importer les fonctions nÃ©cessaires
        from simple_model import load_model_for_prediction, predict_image_from_url
        
        # Charger le modÃ¨le
        model = load_model_for_prediction("plant_classifier", "latest")
        
        if model is None:
            print("âŒ Impossible de charger le modÃ¨le pour l'Ã©valuation locale")
            return {
                'evaluation_method': 'local',
                'accuracy': 0.0,
                'total_tests': len(test_data),
                'successful_tests': 0,
                'correct_predictions': 0,
                'error': 'ModÃ¨le non disponible'
            }
        
        print("âœ… ModÃ¨le chargÃ© pour l'Ã©valuation locale")
        
        correct_predictions = 0
        total_predictions = 0
        predictions = []
        
        for i, test_item in enumerate(test_data):
            try:
                # Faire une prÃ©diction locale
                result = predict_image_from_url(model, test_item['url'])
                
                if result:
                    predicted_label = result['predicted_class']
                    true_label = test_item['label']
                    confidence = result['confidence']
                    
                    is_correct = predicted_label == true_label
                    if is_correct:
                        correct_predictions += 1
                    
                    total_predictions += 1
                    
                    predictions.append({
                        'test_id': i + 1,
                        'url': test_item['url'],
                        'true_label': true_label,
                        'predicted_label': predicted_label,
                        'confidence': confidence,
                        'correct': is_correct
                    })
                    
                    print(f"Test {i+1}: {true_label} -> {predicted_label} ({'âœ…' if is_correct else 'âŒ'}) - {confidence:.2%}")
                    
                else:
                    print(f"âŒ Erreur prÃ©diction locale pour test {i+1}")
                    
            except Exception as e:
                print(f"âŒ Erreur test local {i+1}: {e}")
        
        # Calculer les mÃ©triques
        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
        success_rate = total_predictions / len(test_data) if len(test_data) > 0 else 0
        
        print(f"\nðŸ“Š RÃ©sultats de l'Ã©valuation locale:")
        print(f"  - Tests rÃ©ussis: {total_predictions}/{len(test_data)} ({success_rate:.2%})")
        print(f"  - PrÃ©dictions correctes: {correct_predictions}/{total_predictions}")
        print(f"  - PrÃ©cision: {accuracy:.2%}")
        
        return {
            'evaluation_method': 'local',
            'accuracy': accuracy,
            'total_tests': len(test_data),
            'successful_tests': total_predictions,
            'correct_predictions': correct_predictions,
            'success_rate': success_rate,
            'predictions': predictions
        }
        
    except Exception as e:
        print(f"âŒ Erreur Ã©valuation locale: {e}")
        return {
            'evaluation_method': 'local',
            'accuracy': 0.0,
            'total_tests': len(test_data),
            'successful_tests': 0,
            'correct_predictions': 0,
            'error': str(e)
        }

def check_model_performance(**context):
    """VÃ©rifier si les performances sont acceptables"""
    ti = context['ti']
    eval_result = ti.xcom_pull(task_ids='evaluate_current_model')
    
    if not eval_result:
        raise ValueError("âŒ Aucun rÃ©sultat d'Ã©valuation reÃ§u")
    
    accuracy = eval_result['accuracy']
    method = eval_result['evaluation_method']
    success_rate = eval_result.get('success_rate', 1.0)
    
    min_accuracy = 0.70  # 70% minimum pour la production
    min_success_rate = 0.80  # 80% des tests doivent rÃ©ussir
    
    print(f"ðŸ” Analyse des performances:")
    print(f"  - MÃ©thode d'Ã©valuation: {method}")
    print(f"  - PrÃ©cision: {accuracy:.2%}")
    print(f"  - Taux de rÃ©ussite: {success_rate:.2%}")
    print(f"  - Seuil prÃ©cision: {min_accuracy:.2%}")
    print(f"  - Seuil rÃ©ussite: {min_success_rate:.2%}")
    
    # DÃ©terminer le statut
    if accuracy >= min_accuracy and success_rate >= min_success_rate:
        status = "PERFORMANCE_OK"
        message = "âœ… Performances acceptables"
        needs_retraining = False
    elif accuracy < min_accuracy:
        status = "PERFORMANCE_DEGRADED"
        message = "âš ï¸ PrÃ©cision insuffisante - RÃ©entraÃ®nement recommandÃ©"
        needs_retraining = True
    else:
        status = "SYSTEM_ISSUES"
        message = "âš ï¸ ProblÃ¨mes techniques dÃ©tectÃ©s"
        needs_retraining = False
    
    print(f"  - Status: {message}")
    
    # Recommandations
    recommendations = []
    if accuracy < min_accuracy:
        recommendations.append("RÃ©entraÃ®ner le modÃ¨le avec plus de donnÃ©es")
    if success_rate < min_success_rate:
        recommendations.append("VÃ©rifier la connectivitÃ© et la stabilitÃ© de l'API")
    if not recommendations:
        recommendations.append("Continuer la surveillance")
    
    return {
        'status': status,
        'accuracy': accuracy,
        'success_rate': success_rate,
        'needs_retraining': needs_retraining,
        'evaluation_method': method,
        'recommendations': recommendations,
        'details': eval_result
    }

def generate_report(**context):
    """GÃ©nÃ©rer un rapport d'Ã©valuation"""
    ti = context['ti']
    
    # RÃ©cupÃ©rer les rÃ©sultats
    test_data_result = ti.xcom_pull(task_ids='load_test_data')
    eval_result = ti.xcom_pull(task_ids='evaluate_current_model')
    performance_result = ti.xcom_pull(task_ids='check_model_performance')
    
    print("\n" + "="*60)
    print("ðŸ“Š RAPPORT D'Ã‰VALUATION DU MODÃˆLE")
    print("="*60)
    
    # Informations gÃ©nÃ©rales
    print(f"\nðŸ• Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ðŸ“‹ Source donnÃ©es: {test_data_result.get('data_source', 'N/A')}")
    print(f"ðŸ”¬ MÃ©thode Ã©valuation: {eval_result.get('evaluation_method', 'N/A')}")
    
    # MÃ©triques
    print(f"\nðŸ“ˆ MÃ‰TRIQUES:")
    print(f"  - PrÃ©cision: {eval_result.get('accuracy', 0):.2%}")
    print(f"  - Tests rÃ©ussis: {eval_result.get('successful_tests', 0)}/{eval_result.get('total_tests', 0)}")
    print(f"  - Taux de rÃ©ussite: {eval_result.get('success_rate', 0):.2%}")
    
    # Statut
    print(f"\nðŸŽ¯ STATUT: {performance_result.get('status', 'UNKNOWN')}")
    
    # Recommandations
    print(f"\nðŸ’¡ RECOMMANDATIONS:")
    recommendations = performance_result.get('recommendations', [])
    for i, rec in enumerate(recommendations, 1):
        print(f"  {i}. {rec}")
    
    # DÃ©tails des prÃ©dictions (Ã©chantillon)
    predictions = eval_result.get('predictions', [])
    if predictions:
        print(f"\nðŸ” Ã‰CHANTILLON DE PRÃ‰DICTIONS:")
        for pred in predictions[:5]:  # Afficher les 5 premiers
            status = "âœ…" if pred['correct'] else "âŒ"
            print(f"  {status} {pred['true_label']} -> {pred['predicted_label']} ({pred['confidence']:.2%})")
    
    print("\n" + "="*60)
    
    return {
        'report_generated': True,
        'timestamp': datetime.now().isoformat(),
        'summary': {
            'accuracy': eval_result.get('accuracy', 0),
            'status': performance_result.get('status', 'UNKNOWN'),
            'needs_action': performance_result.get('needs_retraining', False)
        }
    }

# Configuration du DAG
default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'model_evaluation_pipeline',
    default_args=default_args,
    description='Pipeline d\'Ã©valuation des performances du modÃ¨le',
    schedule_interval=timedelta(hours=6),  # Toutes les 6 heures
    catchup=False,
    tags=['ml', 'evaluation', 'monitoring', 'performance'],
    max_active_runs=1,
    doc_md="""
    ## Pipeline d'Ã©valuation des performances
    
    Ce DAG Ã©value rÃ©guliÃ¨rement les performances du modÃ¨le en production.
    
    ### FonctionnalitÃ©s:
    - Ã‰valuation via API ou locale (fallback)
    - Tests sur donnÃ©es rÃ©elles ou URLs par dÃ©faut
    - GÃ©nÃ©ration de rapports dÃ©taillÃ©s
    - Recommandations automatiques
    
    ### Seuils de performance:
    - PrÃ©cision minimale: 70%
    - Taux de rÃ©ussite: 80%
    """
)

# DÃ©finition des tÃ¢ches
load_test_data_task = PythonOperator(
    task_id='load_test_data',
    python_callable=load_test_data,
    provide_context=True,
    dag=dag
)

evaluate_model_task = PythonOperator(
    task_id='evaluate_current_model',
    python_callable=evaluate_current_model,
    provide_context=True,
    dag=dag
)

check_performance_task = PythonOperator(
    task_id='check_model_performance',
    python_callable=check_model_performance,
    provide_context=True,
    dag=dag
)

generate_report_task = PythonOperator(
    task_id='generate_report',
    python_callable=generate_report,
    provide_context=True,
    dag=dag
)

# DÃ©finir les dÃ©pendances
load_test_data_task >> evaluate_model_task >> check_performance_task >> generate_report_task