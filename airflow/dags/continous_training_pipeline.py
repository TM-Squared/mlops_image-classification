from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.sensors.filesystem import FileSensor
from airflow.providers.mysql.hooks.mysql import MySqlHook
from datetime import datetime, timedelta
import sys

sys.path.append('/opt/airflow/ml/training')

def check_new_data(**context):
    """V√©rifier s'il y a de nouvelles donn√©es"""
    mysql_hook = MySqlHook(mysql_conn_id='mysql_default')
    
    # Compter les nouvelles donn√©es depuis la derni√®re ex√©cution
    query = """
    SELECT COUNT(*) as new_count
    FROM plants_data 
    WHERE url_s3 IS NOT NULL 
    AND image_exists = TRUE
    AND created_at > DATE_SUB(NOW(), INTERVAL 1 DAY)
    """
    
    try:
        result = mysql_hook.get_first(query)
        new_count = result[0] if result else 0
    except:
        # Si la colonne created_at n'existe pas, compter toutes les donn√©es
        query = """
        SELECT COUNT(*) as total_count
        FROM plants_data 
        WHERE url_s3 IS NOT NULL 
        AND image_exists = TRUE
        """
        result = mysql_hook.get_first(query)
        new_count = result[0] if result else 0
    
    print(f"üìä Nouvelles donn√©es d√©tect√©es: {new_count}")
    
    min_new_data = 10  # Minimum pour d√©clencher un r√©entra√Ænement
    
    if new_count >= min_new_data:
        print(f"‚úÖ Assez de nouvelles donn√©es ({new_count} >= {min_new_data})")
        return True
    else:
        print(f"‚ùå Pas assez de nouvelles donn√©es ({new_count} < {min_new_data})")
        return False

def retrain_with_new_data(**context):
    """R√©entra√Æner le mod√®le avec toutes les donn√©es disponibles"""
    print("üîÑ R√©entra√Ænement avec nouvelles donn√©es")
    
    # Utiliser le m√™me trainer que pour l'entra√Ænement normal
    from trainer import train_from_database
    
    # Mais avec plus d'√©poques pour l'entra√Ænement continu
    result = train_from_database(num_epochs=10)
    
    print(f"‚úÖ R√©entra√Ænement termin√©:")
    print(f"  - Pr√©cision: {result['accuracy']:.2%}")
    print(f"  - √âchantillons: {result['num_samples']}")
    
    return result

def compare_model_performance(**context):
    """Comparer les performances du nouveau mod√®le"""
    ti = context['ti']
    retraining_result = ti.xcom_pull(task_ids='retrain_with_new_data')
    
    new_accuracy = retraining_result['accuracy']
    
    # Ici, on pourrait comparer avec l'ancien mod√®le
    # Pour simplifier, on consid√®re que le nouveau mod√®le est meilleur si > 70%
    
    threshold = 0.7
    
    if new_accuracy > threshold:
        print(f"‚úÖ Nouveau mod√®le accept√© ({new_accuracy:.2%} > {threshold:.2%})")
        return {
            'decision': 'ACCEPT_NEW_MODEL',
            'new_accuracy': new_accuracy,
            'reason': f'Performance sup√©rieure au seuil ({new_accuracy:.2%})'
        }
    else:
        print(f"‚ùå Nouveau mod√®le rejet√© ({new_accuracy:.2%} <= {threshold:.2%})")
        return {
            'decision': 'REJECT_NEW_MODEL',
            'new_accuracy': new_accuracy,
            'reason': f'Performance insuffisante ({new_accuracy:.2%})'
        }

def deploy_new_model(**context):
    """D√©ployer le nouveau mod√®le si approuv√©"""
    ti = context['ti']
    comparison_result = ti.xcom_pull(task_ids='compare_model_performance')
    
    if comparison_result['decision'] == 'ACCEPT_NEW_MODEL':
        print("üöÄ D√©ploiement du nouveau mod√®le")
        
        # Le mod√®le est d√©j√† dans /shared/models/ gr√¢ce au r√©entra√Ænement
        # On pourrait ici faire une sauvegarde de l'ancien mod√®le
        
        print("üì¶ Nouveau mod√®le d√©ploy√© avec succ√®s")
        return {
            'status': 'DEPLOYED',
            'accuracy': comparison_result['new_accuracy']
        }
    else:
        print("‚ùå Nouveau mod√®le non d√©ploy√©")
        return {
            'status': 'NOT_DEPLOYED',
            'reason': comparison_result['reason']
        }

# Configuration du DAG
default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'continuous_training_pipeline',
    default_args=default_args,
    description='Pipeline d\'entra√Ænement continu avec nouvelles donn√©es',
    schedule_interval=timedelta(days=1),  # Quotidien
    catchup=False,
    tags=['ml', 'continuous', 'training', 'automation'],
    max_active_runs=1
)

# D√©finition des t√¢ches
check_new_data_task = PythonOperator(
    task_id='check_new_data',
    python_callable=check_new_data,
    provide_context=True,
    dag=dag
)

retrain_task = PythonOperator(
    task_id='retrain_with_new_data',
    python_callable=retrain_with_new_data,
    provide_context=True,
    dag=dag
)

compare_performance_task = PythonOperator(
    task_id='compare_model_performance',
    python_callable=compare_model_performance,
    provide_context=True,
    dag=dag
)

deploy_new_model_task = PythonOperator(
    task_id='deploy_new_model',
    python_callable=deploy_new_model,
    provide_context=True,
    dag=dag
)

# D√©finir les d√©pendances avec branchement conditionnel
check_new_data_task >> retrain_task >> compare_performance_task >> deploy_new_model_task